{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s49CZnYW0Hhy"
      },
      "outputs": [],
      "source": [
        "# Tools\n",
        "import pandas as pd, numpy as np\n",
        "\n",
        "# Dimensionality Reduction\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# Visualizations\n",
        "import matplotlib.pyplot as plt, seaborn as sns\n",
        "\n",
        "# Feature Importance\n",
        "import shap\n",
        "shap.initjs()\n",
        "\n",
        "# Model Scoring\n",
        "from sklearn.metrics import classification_report, balanced_accuracy_score, f1_score\n",
        "\n",
        "# Imbalance\n",
        "from imblearn.under_sampling import TomekLinks, RandomUnderSampler, RepeatedEditedNearestNeighbours, CondensedNearestNeighbour\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "# Other\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler, MinMaxScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.impute import KNNImputer, SimpleImputer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JGBtNuIJ2qcx"
      },
      "outputs": [],
      "source": [
        "# Data Exploration 1\n",
        "\n",
        "\"\"\"\n",
        "TODO: \n",
        "1. Import the data\n",
        "2. Answer the following questions about the data:\n",
        "  a. What are my column names? \n",
        "  b. How many unique values does each column have?\n",
        "  c. What type of data is each column (continuous, categorical, text)?\n",
        "3. Create a dict of k,v pairs where k == column name, v == column type (1 of continuous, categorical, text)\n",
        "\n",
        "Resources\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html\n",
        "https://www.w3schools.com/python/python_dictionaries.asp\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ho7na14t2s7S"
      },
      "outputs": [],
      "source": [
        "# Data Exploration 2\n",
        "\n",
        "\"\"\"\n",
        "TODO: \n",
        "1. Visualize the data (using seaborn)\n",
        "  a. For categorical, create histograms w/ KDE estimation, color by class\n",
        "  b. For continuous, create scatter (or cat) plot w/ class\n",
        "  c. For continuous, create histograms w/ KDE estimation, color by class\n",
        "\n",
        "Resources\n",
        "https://seaborn.pydata.org/generated/seaborn.histplot.html\n",
        "https://seaborn.pydata.org/generated/seaborn.catplot.html\n",
        "\"\"\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cwp0qv7MJsUZ"
      },
      "outputs": [],
      "source": [
        "# Data Exploration 4\n",
        "\"\"\"\n",
        "TODO:\n",
        "1. Run correlation analysis on your columns (Spearman Correlation)\n",
        "2. Plot the correlation heatmap\n",
        "\n",
        "Resources\n",
        "https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.corr.html\n",
        "https://seaborn.pydata.org/generated/seaborn.heatmap.html\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uwyDEksUJE0e"
      },
      "outputs": [],
      "source": [
        "# Data Exploration 5\n",
        "\"\"\"\n",
        "TODO:\n",
        "1. Figure out if any columns should not be taken into account for analysis. Typically:\n",
        "  a. If column has a single unique value\n",
        "  b. If column is categorical, but has all unique values\n",
        "  c. If column is more than 33% NaN, null, or missing\n",
        "  d. If column is highly correlated with another\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T2W3lfr5EJPH"
      },
      "outputs": [],
      "source": [
        "# Data Exploration 6\n",
        "\"\"\"\n",
        "TODO:\n",
        "1. Figure out which columns have missing (NaN, null or blank) data (IF ANY -- IF THERE IS NO MISSING DATA, SKIP THIS STEP!)\n",
        "2. Create 3 separate dataframes:\n",
        "  a. 1 which uses KNN Imputing to fill in missing values\n",
        "  b. 1 which uses the most common value in the column, for that label, to fill in missing values\n",
        "  c. 1 which fills in all values with 0\n",
        "\n",
        "Resources\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.impute.KNNImputer.html\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.impute.SimpleImputer.html\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "60Ol1WsbL2GZ"
      },
      "outputs": [],
      "source": [
        "# Other Feature Engineering as necessary"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i0Acz9YwKKw2"
      },
      "outputs": [],
      "source": [
        "# Class Evaluation 1\n",
        "\"\"\"\n",
        "TODO:\n",
        "1. Evaluate if there is a difference in classes\n",
        "\n",
        "Resources\n",
        "https://www.hackerrank.com/challenges/collections-counter/problem\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JbGqfSyYLmC-"
      },
      "outputs": [],
      "source": [
        "# Class Evaluation 2\n",
        "\"\"\"\n",
        "TODO:\n",
        "If and only if there is a class imbalance:\n",
        "NOTE: Do this on the original dataframes themselves\n",
        "1. Remove Tomek Links\n",
        "\n",
        "2. Create Under Sampling DFs\n",
        "NOTE: You should create NEW dataframes, each of the 3, with each of these techniques\n",
        "  a. RandomUnderSampler\n",
        "  b. RepeatedEdited Nearest Neighbors\n",
        "  c. Condensed Nearest Neighbors\n",
        "\n",
        "3. Create Over Sampling Objects\n",
        "  a. ADASYN\n",
        "\n",
        "Resources\n",
        "\n",
        "Tomek Links\n",
        "https://imbalanced-learn.org/dev/references/generated/imblearn.under_sampling.TomekLinks.html\n",
        "https://imbalanced-learn.org/dev/auto_examples/under-sampling/plot_illustration_tomek_links.html#sphx-glr-auto-examples-under-sampling-plot-illustration-tomek-links-py\n",
        "\n",
        "Under Sampling\n",
        "https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RandomUnderSampler.html\n",
        "https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.RepeatedEditedNearestNeighbours.html\n",
        "https://imbalanced-learn.org/stable/references/generated/imblearn.under_sampling.CondensedNearestNeighbour.html\n",
        "https://imbalanced-learn.org/dev/references/generated/imblearn.over_sampling.ADASYN.html\n",
        "\n",
        "NOTE: \n",
        "\n",
        "When you are complete, you should have 15 datasets maximum:\n",
        "knn_imputed_random_under_sampler\n",
        "average_val_random_under_sampler\n",
        "zero_random_under_sampler\n",
        "\n",
        "knn_imputed_repeated_edited_nn\n",
        "average_val_repeated_edited_nn\n",
        "zero_repeated_edited_nn\n",
        "\n",
        "knn_imputed_condensed_nn\n",
        "average_val_condensed_nn\n",
        "zero_condensed_nn\n",
        "\n",
        "knn_imputed_adasyn\n",
        "average_val_adasyn\n",
        "zero_adasyn\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BZkIQYg5NqW4"
      },
      "outputs": [],
      "source": [
        "# Train test splits\n",
        "\"\"\"\n",
        "TODO:\n",
        "1. For each dataset, split into training and testing splits\n",
        "  a. Make sure to stratify split it \n",
        "  b. 80% train, 20% test\n",
        "\n",
        "Hint:\n",
        "One thing we want to do here is ensure our split datasets maintain the same distribution of our feature data. Meaning, we don't want a column that has an average value of 5 to be split into train/test, where the train average\n",
        "is 4 and the test average is 14. That would not be representative of the entire dataset. So you need to come up with a way to split the data in such a fashion that it maintains distributions...\n",
        "\n",
        "Hint Hint:\n",
        "training_data = df.groupby('target').sample(frac=training_split, random_state=8)\n",
        "testing_data = df[~df.index.isin(training_data.index)]\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare Training and Testing Data\n",
        "\"\"\"\n",
        "\n",
        "\"\"\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TODO\n",
        "This section will include future sections that will begin working on modeling (classification), anomaly detection, and ultimately explainable outputs using MAPIE and SHAP"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "It4StfamO1t4"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
